{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearExpectedSarsaFunction():\n",
    "    def __init__(self, n_features, action_space = 4, weights=None, default=0.0, lr = 0.001, gamma = 0.99, epsilon = 0.91, annealing_coefficient = 0.99999999 ):\n",
    "        #In this case, features represents the states the agent see\n",
    "        #n_features should be the number of states that the agent sees\n",
    "        #action_space should be the number of actions the agent can take\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.action_space = action_space\n",
    "        self.n_actions = action_space ##I will make sure that n_actions and action_space is different\n",
    "        self.lr = lr #learning rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.annealing_coefficient = annealing_coefficient\n",
    "\n",
    "        if weights == None:\n",
    "            self.weights = np.array(\n",
    "                [\n",
    "                    [default] * n_features\n",
    "                    for _ in range(0, self.action_space)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def update(self, curent_stacked_feature, action, next_stacked_features, reward, done, possible_next_actions):\n",
    "        # update the weights\n",
    "\n",
    "        q = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            q += self.weights[i][action] * curent_stacked_feature[i]\n",
    "\n",
    "        self.take_action(possible_next_actions, next_stacked_features)\n",
    "\n",
    "        q_nexts = np.zeros(len(possible_next_actions))\n",
    "        for i in range(len(self.weights)):\n",
    "            for j in range(len(possible_next_actions)):\n",
    "                q_nexts[j] += self.weights[i][possible_next_actions[j]] * next_stacked_features[i]\n",
    "\n",
    "        #Starting here for expected value of sarsa\n",
    "        max_q = np.max(q_nexts)\n",
    "        n_max_q = 0\n",
    "\n",
    "        for q_next in q_nexts: #determining how many max q_values\n",
    "            if q_next == max_q:\n",
    "                n_max_q +=1\n",
    "\n",
    "        #probability distribution\n",
    "        non_greedy_action_prob = self.epsilon/len(possible_next_actions)\n",
    "        greedy_action_prob = (1-self.epsilon)/n_max_q + non_greedy_action_prob\n",
    "\n",
    "        expected_q = 0\n",
    "        sum_prop = 0\n",
    "        for i in range(self.n_actions):\n",
    "            if(q_nexts[i] == max_q):\n",
    "                expected_q += greedy_action_prob * q_nexts[i]\n",
    "                sum_prop += greedy_action_prob\n",
    "            else:\n",
    "                expected_q += non_greedy_action_prob * q_nexts[i]\n",
    "                sum_prop += non_greedy_action_prob\n",
    "\n",
    "        #TD based on expected sarsa\n",
    "        td_error = reward + self.gamma * expected_q*(1-done) - q\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i][action] += self.lr * td_error * curent_stacked_feature[i]\n",
    "\n",
    "        #annealing epsilon\n",
    "        if self.epsilon > 0.1:\n",
    "            self.epsilon *= self.annealing_coefficient\n",
    "    \n",
    "    def take_action(self, possible_actions, curent_stacked_feature):\n",
    "        if(np.random.random()  < self.epsilon):\n",
    "            return np.random.choice(possible_actions)\n",
    "        else:\n",
    "            q = np.zeros(len(possible_actions))\n",
    "     \n",
    "            for i in range(len(curent_stacked_feature)):\n",
    "                for j in range( len(possible_actions) ):\n",
    "                    q[j] += self.weights[i][ possible_actions[j] ] * curent_stacked_feature[i]\n",
    "\n",
    "            arg_max_index = np.argmax(q)\n",
    "            return possible_actions[arg_max_index]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
